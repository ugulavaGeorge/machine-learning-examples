{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and reading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shlex, subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from collections import Counter\n",
    "from tensorflow.contrib import rnn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "data = pd.read_csv('text_emotion.csv')\n",
    "names = data.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Preprocessing, select 5 most frequent labels and map them to float number\n",
    "#for purpose of easy labeling for nn.\n",
    "baseline = ['sadness', 'worry', 'happiness', 'love', 'neutral']\n",
    "data = data[[(sentiment in baseline) for sentiment in data['sentiment']]]\n",
    "data['sentiment'] = data['sentiment'].map({'sadness' : 1.0, 'worry' : 2.0,\n",
    "                        'happiness' : 3.0, 'love' : 4.0, 'neutral' : 5.0})\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def draw_histogram(data):\n",
    "    labels,values = zip(*Counter(data['sentiment']).items())\n",
    "    indexes = np.arange(len(labels))\n",
    "    width = 0.5\n",
    "    plt.xticks(indexes + width * 0.5, labels, rotation = 'vertical')\n",
    "    plt.bar(indexes, values, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#distribution of train data\n",
    "draw_histogram(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#distribution of test_data\n",
    "draw_histogram(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = None #PorterStemmer()\n",
    "    tokens = word_tokenize(sentence)\n",
    "    lemmas = []\n",
    "    skip = False\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] == '@':\n",
    "            skip = True\n",
    "        elif skip:\n",
    "            skip = False\n",
    "        else:\n",
    "            lemmas.append(lemmatizer.lemmatize(tokens[i])) \n",
    "    stems = []\n",
    "    if stemmer != None:\n",
    "        for lemma in lemmas:\n",
    "            stems.append(stemmer.stem(lemma))\n",
    "        return \" \".join(stems)\n",
    "    return \" \".join(lemmas)\n",
    "            \n",
    "#converting data to fit the shape appropriate for rnn which expects:\n",
    "#tensor[B,T....] where B - batch size, T - Timesteps, and then goes features\n",
    "data['content'] = list(map(preprocess_sentence, data['content']))\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "X_train_TF = pd.DataFrame(vectorizer.fit_transform(train_data['content']).todense())\n",
    "X_test_TF = pd.DataFrame(vectorizer.transform(test_data['content']).todense())\n",
    "Y_train = train_data['sentiment']\n",
    "Y_test = test_data['sentiment']\n",
    "print(X_test_TF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " \"\"\"takes a list of tokens for which returns following features as a list\n",
    "1 - number of exlamation marks\n",
    "2 - number of question marks\n",
    "3 - number of quotes\n",
    "4 - number of words in upper case\n",
    "5 - boolean value if in the tokens there is containing 3+ single-character repetion.\"\"\"\n",
    "def get_second_feature_block_values(tokens):\n",
    "    exl_marks_number  = 0\n",
    "    ques_marks_number = 0\n",
    "    quotes_number = 0\n",
    "    upper_case_words = 0\n",
    "    vowel_repetition = False\n",
    "    for word in tokens:\n",
    "        exl_marks_number += word.count(\"!\")\n",
    "        ques_marks_number += word.count(\"?\")\n",
    "        quotes_number += word.count('\"')\n",
    "        if word.isupper():\n",
    "            upper_case_words += 1\n",
    "        if contains_vowel_repetition(word):\n",
    "            vowel_repetition = True\n",
    "    return [exl_marks_number, ques_marks_number, quotes_number, upper_case_words, vowel_repetition]\n",
    "\n",
    "\"\"\"returns true if the word containing 3+ single-character repetion\"\"\"    \n",
    "def contains_vowel_repetition(word):\n",
    "    result = False\n",
    "    for i in range(len(word)-2):\n",
    "        if word[i] == word[i+1] and word[i] == word[i+2]:\n",
    "            result = True\n",
    "    return result\n",
    "\n",
    "\"\"\"takes a list of tokens for which returns following features as a list\n",
    "1 - total positive score\n",
    "2 - total negative score\n",
    "3 - number of highly emotional negative words(w_score <= -3)\n",
    "4 - number of highly emotional positive words(w_score >= 3)\n",
    "5 - ratio of emotional words.\"\"\"\n",
    "def get_first_features_block_values(tokens):\n",
    "    sentence = ' '.join(tokens)\n",
    "    total_scores = get_sentiment_rate(sentence)\n",
    "    total_positive_score = total_scores[0]\n",
    "    total_negative_score = total_scores[1]\n",
    "    hepw_count = 0\n",
    "    henw_count = 0\n",
    "    for token in tokens:\n",
    "        sentiment_rate = get_sentiment_rate(token)\n",
    "        if sentiment_rate[0] > sentiment_rate[1] and sentiment_rate[0] >= 3:\n",
    "            hepw_count += 1\n",
    "        elif sentiment_rate[0] < sentiment_rate[1] and sentiment_rate[1]>= 3:\n",
    "            henw_count += 1\n",
    "    if total_positive_score <=1 and total_negative_score <=1:\n",
    "        ratio = 0\n",
    "    else :\n",
    "        ratio = (total_positive_score - total_negative_score)/(total_positive_score + total_negative_score)\n",
    "    return [total_positive_score, total_negative_score, hepw_count, henw_count, ratio]\n",
    "\n",
    "\"\"\"for specified string returns its sentiStrength score as a pair of numbers\n",
    "first corresponds to positive rate, second for negative.\"\"\"\n",
    "def get_sentiment_rate(sourceString):\n",
    "    process = subprocess.Popen(shlex.split(\"java -jar /home/george/ipython/SentiStrength.jar stdin sentidata \\\n",
    "        /home/george/ipython/data/\"), stdin = subprocess.PIPE, stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "    stdout_response, stderr_responce = process.communicate(sourceString.replace(\" \",\"+\").encode())\n",
    "    stdout_response = [int(s) for s in str(stdout_response) if s.isdigit()]\n",
    "    return stdout_response\n",
    "\n",
    "def get_wrapped_features(tokens):\n",
    "    first_block = get_first_features_block_values(tokens)\n",
    "    second_block = get_second_feature_block_values(tokens)\n",
    "    first_block.extend(second_block)\n",
    "    return first_block\n",
    "\n",
    "def extend_data_frame(data_frame, number_features):\n",
    "    features_num = data_frame.shape[1]\n",
    "    for i in range(number_features):\n",
    "        position = features_num + 1 + i\n",
    "        data_frame.assign(position = np.nan)\n",
    "    return data_frame\n",
    "\n",
    "def join_features(data_frame, features, index):\n",
    "    features_num = data_frame.shape[1]\n",
    "    for i in range(len(features)):\n",
    "        data_frame.set_value(index, features_num + 1 + i, features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_sets = []\n",
    "i = 0\n",
    "k = 0\n",
    "for content in test_data['content']:\n",
    "    features_sets.append(get_wrapped_features(word_tokenize(content)))\n",
    "    k += 1\n",
    "    if k % 100 == 0:\n",
    "        print(k//100)\n",
    "print(\"features done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = extend_data_frame(X_test_TF, 10)\n",
    "print(\"frame extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_for_test = pd.DataFrame(features_sets)\n",
    "features_for_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data.to_csv(\"train_raw_data.csv\", sep='\\t')\n",
    "test_data.to_csv(\"test_raw_data.csv\", sep='\\t')\n",
    "Y_train.to_csv(\"y_train.csv\", sep='\\t')\n",
    "Y_test.to_csv(\"y_test.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for feature_set in features_sets:\n",
    "    join_features(X_test, feature_set, i)\n",
    "    i += 1\n",
    "    if i % 100 == 0:\n",
    "        print(i//100)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_sets_tr = []\n",
    "it = 0\n",
    "kt = 0\n",
    "for content in train_data['content'][:6262]:\n",
    "    features_sets_tr.append(get_wrapped_features(word_tokenize(content)))\n",
    "    kt += 1\n",
    "    if kt % 100 == 0:\n",
    "        print(kt)\n",
    "print(\"features done\")\n",
    "\n",
    "X_train = extend_data_frame(X_train_TF, 10)\n",
    "print(\"frame extended\")\n",
    "for feature_set in features_sets_tr:\n",
    "    join_features(X_train, feature_set, it)\n",
    "    it += 1\n",
    "    if it % 100 == 0:\n",
    "        print(it//100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_for_train.to_csv('features_for_train.csv', mode='a', header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
